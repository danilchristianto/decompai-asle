{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import sys\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mps.empty_cache()\n",
    "\n",
    "# Check if MPS (Metal Performance Shaders) is available and set the device accordingly\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "# Load tokenizer and model from Hugging Face Hub and move model to the MPS device if available\n",
    "model_reference = \"LLM4Binary/llm4decompile-1.3b-v2\"\n",
    "# model_reference = \"LLM4Binary/llm4decompile-6.7b-v1.5\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_reference)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_reference)\n",
    "\n",
    "# Convert model to FP16 to reduce memory usage\n",
    "model = model.half().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompile_with_llm4decompile(disassembled_code):\n",
    "    # Tokenize the input text and add attention mask\n",
    "    inputs = tokenizer(disassembled_code, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "    # Setting pad_token_id to eos_token_id, since this is the default behavior for many generative models\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "    # Print the number of tokens in the input\n",
    "    print(f\"Number of tokens in input: {inputs.input_ids.size(1)}\")\n",
    "    \n",
    "    # Generate decompiled code with attention mask\n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        attention_mask=inputs.attention_mask,\n",
    "        max_new_tokens=200\n",
    "    )\n",
    "    decompiled_code = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Save the decompiled code for inspection\n",
    "    decompiled_path = os.path.join(utils.WORKSPACE_DIR, \"decompiled_code.c\")\n",
    "    with open(decompiled_path, \"w\") as f:\n",
    "        f.write(decompiled_code)\n",
    "    \n",
    "    return decompiled_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(input_path, function_name):\n",
    "    # Copy the C code or binary to the workspace for reference\n",
    "    try:\n",
    "        disassembled_code = utils.disassemble(input_path, function_name)\n",
    "\n",
    "        print(disassembled_code)\n",
    "        \n",
    "        prompt = f\"# This is the assembly code:\\n{disassembled_code}\\n# What is the source code?\\n\"\n",
    "        \n",
    "        # Decompile with LLM4Decompile\n",
    "        print(\"\\nDecompiling...\")\n",
    "        decompiled_code = decompile_with_llm4decompile(prompt)\n",
    "        \n",
    "        # Remove input from the decompiled code\n",
    "        decompiled_code = decompiled_code[len(prompt):].strip()\n",
    "        \n",
    "        print(\"\\nDecompiled Code:\")\n",
    "        print(decompiled_code)\n",
    "    \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error during processing: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_code_path = \"c_code/fibonacci.c\"\n",
    "function_name = \"func0\"\n",
    "process_file(c_code_path, function_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
